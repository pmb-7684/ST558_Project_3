---
title: "Project 3: Predictive Models on Online Popularity Data"
author: "Smitali Paknaik and Paula Bailey"
date: "2022-11-04"
params:
   channel: "data_channel_is_lifestyle"

---

## Intro

Set up for knit process.  Set universal chunk code options, so it will not be necessary to update each chunk individually.[article](https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE, fig.align='center')

```


  * The following packages are required for creating predictive models.

1. `dplyr` - A part of the `tidyverse` used for manipulating data
2. `tidyr` - A part of the `tidyverse` used for data cleaning
3. `ggplot2` - A part of the `tidyverse` used for creating graphics
4. 


```{r lib, include = FALSE}

library(caret)      #training/test (splitting)
library(glmnet)     #best subset selection
library(GGally)     #create ggcorr and ggpairs plots
library(ggplot2)
library(leaps)      #identify different best models of different sizes
library(markdown)
library(MASS)       #access to forward and backward selection algorithms
library(purrr)
library(tidyverse)  #tidyverse set of packages and functions

```

We used read.csv to load in the data.  The UCI site mentioned that `url` and `timedelta` are non-predictive variables, so we will remove them from our data set.  Afterwards, I checked to validate the data set contained no missed values.  



## Load Data, Rename Variables, and check for NAs
```{r}
data <- read.csv("OnlineNewsPopularity.csv") %>% 
                              rename(#`Lifestyle` =`data_channel_is_lifestyle`,
                                    #Entertainment = `data_channel_is_entertainment`,
                                    #Business      = `data_channel_is_bus`,
                                    #SocialMedia = `data_channel_is_socmed`,
                                    #Technology    = `data_channel_is_tech`,
                                    #World       = `data_channel_is_world`,
                                    Monday      = `weekday_is_monday`,
                                    Tuesday     = `weekday_is_tuesday`,
                                    Wednesday   = `weekday_is_wednesday`,
                                    Thursday    = `weekday_is_thursday`,
                                    Friday      = `weekday_is_friday`,
                                    Saturday    = `weekday_is_saturday`,
                                    Sunday      = `weekday_is_sunday`) %>% 
                                    dplyr::select(-url, -timedelta)

#check for missing values
anyNA(data)  
```

For the automation, it will be easier if all the channels are in one column.  I used `pivot_longer()` to pivot columns: data_channel_is_lifestyle,data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world from wide to long format.

Initially, I planned to rename the columns in the data set; however, it took me a while to figure it out, but the automation does not work when I renamed the columns.  I will review it again later.

```{r}
dataPivot <- data %>% pivot_longer(cols = c("data_channel_is_lifestyle", "data_channel_is_entertainment", "data_channel_is_bus","data_channel_is_socmed","data_channel_is_tech", "data_channel_is_world"),names_to = "channel",values_to = "Temp") 

```

```{r}
newData <- dataPivot %>% filter(Temp != 0) %>% dplyr::select(-Temp)
```


Now, the individual channel columns are combined into one column named channel.  The Temp value represents if an article exists for that particular change.  For the final data set, we will remove any values with 0.  The final data set has 33510 obs....**double check that you are not loosing too much data**  The original data set had 39644 obs, so we lost over 6000 observations... hmm.,, I'm fine with pivot, but not sure about removing missing information for day of the week.... again, come back.


```{r}
dataPivot1 <- dataPivot %>% pivot_longer(c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"), names_to = "weekday",values_to = "Temp1") 
```


These are my notes, so it seems like I'm jumping around.  I will perform pivot on "weekday" information as well. Afterwards, remove Temp2 which represents articles without a day of the week listed.  

```{r}
newData <- dataPivot1 %>% filter(Temp1 != 0) %>% dplyr::select(-Temp1)
```


```{r}
unique(newData$channel)
```


## Part of the automation
```{r}
selectChannel <- newData %>% filter(channel == params$channel)
```


## Train and Test on practice automated
```{r}

set.seed(21)
trainIndex <- createDataPartition(selectChannel$shares, p = 0.7, list = FALSE)
selectTrain <- selectChannel[trainIndex, ]
selectTest <- selectChannel[-trainIndex, ]

```


## Summaries on Training Data
```{r}
selectTrain %>% dplyr::select(shares, starts_with("rate"), starts_with("avg") ) %>%summary()
```
The distribution of several quantitative variables - shares, (response variable), rate_positive_words, rate_negative_words, avg_positive_polarity, and avg_negative_polarity.

We know the following about the distribution (shape):

    Right-skewed if mean is greater than median.    
    Left-skewed if mean is less than median.    
    Normal if mean equals to median.    

Share and positive words - hmmm, I expected something a bit more interesting....I figures there would be an uptick in articles shared.  Come back... JUst creating a bunch of plots to see what is interesting.

## Plot
```{r}
ggplot(selectTrain, aes(x=rate_positive_words, y = shares ))+geom_point()
```


## Contingency Tables

Articles per Channel vs Weekday
```{r}
table(selectTrain$channel, selectTrain$weekday)
```


## Correlations

We want to remove any predictor variables that are highly correlated to the response variable as called multicollinearity.  if variables have this characteristic it can lead to skewed or misleading results. We will create grouping of ten predictor variables to look at the relationship with our response variable share.

Looking at the results from the `ggcorr`, we do not see any highly correlated relationships.  if there was such relationship, it would be 1 or orange for highly positive correlated and -1 or blue for highly negative correlated.

**need to make corr plot look better - see if can change font on label**

```{r}
ggcorr(selectTrain%>% dplyr::select(kw_min_min, kw_max_min, kw_avg_min, kw_min_max, kw_max_max,  kw_avg_max, kw_min_avg, kw_max_avg, kw_avg_avg, shares),label_round = 2, label = "TRUE")
```

## Linear Regression

Linear regression is a method to understand the relationship between a response variables Y and one or more predictor variables x, x1, x2, etc.  This method creates a line that best fits the data called "least known regression line"

For a Simple Linear Regression, we one response variable and one predictor variables. It uses the formula y = b0 + b1x, where....

For Multiple Linear Regression, we have one response variable and any number of predictor variable. It uses the formula Y = B0 + B1X1 + B2X2 + â€¦ + BpXp + E, where .....

Also add the 4 assumptions - linear relationship, independence, homoscedasticity, and Normality.


```{r}


```

## Random Forest

Similar to bagged trees, but created using multiple trees and averages the final results.
The random forest model is created using `rf` as the method and `mtry` for the tuning grid from 1 to 11.
```{r}

```


Test area for params
to automatic Rmarkdown, see topic 2 notes, beginning on page 329: rewatch video

## Automation of Building Six Reports 
```{r, eval = FALSE}
selectID <- unique(newData$channel)  

output_file <- paste0(selectID, "Analysis.html")   #should be .md for the project

params = lapply(selectID, FUN = function(x){list(channel = x)})

reports <- tibble(output_file, params)

library(rmarkdown)

apply(reports, MARGIN = 1,
      FUN = function(x){
        render(input = "./TestOne.Rmd",
               output_format = "github_document", 
               output_file = x[[1]], 
               params = x[[2]])
      })



```



