---
title: "Project 3 - Predicitive Models"
author: "Smitali Paknaik and Paula Bailey"
date: "2022-11-04"
params:
   channel: "data_channel_is_lifestyle"
---


Set up for knit process.  Set universal chunk code options, so it will not be necessary to update each chunk individually.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE, fig.align='center')

```

# Introduction section
## Introduction 

The main goal of this page is to analyze OnlineNewsPopularity data using Exploratory Data Analysis and apply regression and ensemble methods to predict the response variable ~share using a set of predictor variables optimized using set of variable selection methods. 

At first step the data is to shape the data to get the various data_channel_groups together. This is needed in order to get proper filtering of the required channels in the automation script. The EDA is performed on each group using graphs, tables and heatmaps. 
 
The next step includes creating train and test data for train fit and test prediction. The next steps includes variable selection /pre-processing of data (if needed) and running 2 regression models, Linear models are the first step of performing model fit and predictions. The data may be easily explained by a set of linear equations and may not require complex algorithms that can consume time, cost and may require computation speed. Hence, regression models often are a great start to run predictions.
 
The Other methods evaluated here are Random Forest and Boosted Tree method. Both are types of ensemble methods that use tree based model to evaluate future values. These methods do not have any specified methodology in model fit. They work on the best statistic obtained while constructing the model fit. Theoretically ensemble methods are said to have a good prediction, and using this data it will be evaluated on what categories how these models perform.

In the end for each channel category, all the 4 models are compared and best model that suits this category of channel is noted.  

#### About Data and Data Preparation
OnlineNewsPopularity data has following useful variables:
_from models we do for eda and regression/ensemble

The channel categories evaluated are : 'lifestyle','entertainment','bus','socmed','tech' and 'world'.

The csv file is imported read using `read_csv`. As we have automated the analysis, we need each channel to be fed into the R code we have and get the analysis and predictions as a different file. The channel categories seem to be in column formats so they have been converted into long form using `pivot_longer`. 
The required category is then easily filter by passing the channel variables into the filter code.

Irrelevant columns removed - Note after all the work and cleaning
The Exploratory analysis and Modelling was carried on the remaining set of select variables. 
 
 
## Package Imports 

  * The following packages are required for creating predictive models.

1. `dplyr` - A part of the `tidyverse` used for manipulating data
2. `tidyr` - A part of the `tidyverse` used for data cleaning
3. `ggplot2`- A part of the `tidyverse` used for creating graphics
4  `caret` - To run the Regression and ensemble methods with Train/Split and cross validation.
5.  `knitr`: To get nice table printing formats, mainly for the contingency tables.

```{r lib, include = FALSE}

library(caret)      #training/test (splitting)
library(glmnet)     #best subset selection
library(GGally)     #create ggcorr and ggpairs plots
library(ggplot2)
library(leaps)      #identify different best models of different sizes
library(markdown)
library(MASS)       #access to forward and backward selection algorithms
library(purrr)
library(tidyverse)  #tidyverse set of packages and functions
library(randomForest)

```

We used read.csv to load in the data.  The UCI site mentioned that `url` and `timedelta` are non-predictive variables, so we will remove them from our data set.  Afterwards, I checked to validate the data set contained no missed values.  

# ``r params$channel`` Analysis
# Load Data, Rename Variables and check for NAs 
```{r}
data <- read.csv("OnlineNewsPopularity.csv") %>% 
                              rename(
                                    Monday      = `weekday_is_monday`,
                                    Tuesday     = `weekday_is_tuesday`,
                                    Wednesday   = `weekday_is_wednesday`,
                                    Thursday    = `weekday_is_thursday`,
                                    Friday      = `weekday_is_friday`,
                                    Saturday    = `weekday_is_saturday`,
                                    Sunday      = `weekday_is_sunday`) %>% 
                                    dplyr::select(-url, -timedelta)

#check for missing values
anyNA(data)  
```

We used read.csv to load in the data.  The UCI site mentioned that `url` and `timedelta` are non-predictive variables, so we will remove them from our data set.  Afterwards, checked to validate the data set contained no missed values.  anyNA(data) returned FALSE, so the file has no missing data.


For the automation, it will be easier if all the channels (ie data_channel_is_*) are in one column.  I used `pivot_longer()` to pivot columns: data_channel_is_lifestyle,data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world from wide to long format.
```{r}
dataPivot <- data %>% pivot_longer(cols = c("data_channel_is_lifestyle", "data_channel_is_entertainment", "data_channel_is_bus","data_channel_is_socmed","data_channel_is_tech", "data_channel_is_world"),names_to = "channel",values_to = "Temp") 


newData <- dataPivot %>% filter(Temp != 0) %>% dplyr::select(-Temp)
```

Now, the individual channel columns are combined into one column named channel.  The Temp value represents if an article exists for that particular change.  For the final data set, we will remove any values with 0.  Next, we will perform the same pivot_longer() process on the days of the week

```{r}
dataPivot <- newData %>% pivot_longer(c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"), names_to = "weekday",values_to = "Temp1") 

newData <- dataPivot %>% filter(Temp1 != 0) %>% dplyr::select(-Temp1)
```

# Create Train and Test Sets

Part of the automation when we run the `Render_Code.R`, it selects each channel individually to complete the analysis.
```{r}
selectChannel <- newData %>% filter(channel == params$channel)
```


To make the data reproducible, we used set seed to 21 and created the training and testing set with a 70% split.
```{r}

set.seed(21)

trainIndex <- createDataPartition(selectChannel$shares, p = 0.7, list = FALSE)
selectTrain <- selectChannel[trainIndex, ]
selectTest <- selectChannel[-trainIndex, ]
```

# Exploratory Data Analysis
## Summary on Training Data

```{r}
str(selectTrain)
```
Viewing the structure of the data, the data types seem to be fine.

```{r include=FALSE}
summary(selectTrain)
```

```{r}
selectTrain %>% dplyr::select(shares, starts_with("rate")) %>% summary()
```
The summary provides us with information about the distribution (shape) of  shares, (response variable), rate_positive_words, and rate_negative_word.

    If mean is greater than median, then Right-skewed with outliers towards the right tail.    
    If mean is less than median, then Left-skewed with outliers towards the left tail.    
    If mean equals to median, then Normal distribtuion.    


## Plots of the Training Data

Note:  Due to the extreme minimize and maximize values in our response variable, shares, we transformed the data by applying Log(). Log() allows us to address tge differences in magnitude throughout the share variable.  The results visually are much better.

```{r scatter share}
g <- ggplot(selectTrain, aes(x=rate_positive_words, y = log(shares)))+geom_point() 
g + geom_jitter(aes(color = as.factor(weekday))) + 
	      labs(x = "y", 
	           title = "Rate of Positive Words")
```
We can inspect the trend of shares as a function of the positive word rate. If the points
show an upward trend, then articles with more positive words tend to be shared more often.
If we see a negative trend then articles with more positive words tend to be shared less often.


```{r histogram shares}
g <- ggplot(selectTrain, aes(x = log(shares)))
g + geom_histogram(bins = sqrt(nrow(selectTrain)))  + 
	      labs(title = "Histogram of Shares")
```
We can inspect the shape of the response variable, shares. If the share of the histogram is symmetrical, then shares has a normal distribution with the shares evenly spread throughout. If the shape is left skewed, then the tail which contains a most of the outliers will be extended to the left.  If the shape is right skewed, then the tail which contains a most of the outliers will be extended to the right.

```{r boxplot shares}

g <- ggplot(selectTrain) 
 g + aes(x = weekday, y = log(shares)) +
  geom_boxplot(varwidth = TRUE) + 
  geom_jitter(alpha = 0.25, width = 0.2,aes(color = as.factor(weekday)))  + 
	 labs(title = "Box Plot of Shares Per Day")
```
We can inspect the distribution of shares as a function of the day of the week. If the points
are within the body of the box, then articles shared are within the 1st and 3rd quartile.
If we see a articles outside of the whiskers of the boxplot, it represents outliers in the data.


## Contingency Tables

Articles per Channel vs Weekday
```{r}
table(selectTrain$channel, selectTrain$weekday)
```
We can inspect the distribution of shares as a function of the day of the week. 

Day of the Week vs Number of Keywords
```{r}
table(selectTrain$weekday, selectTrain$num_keywords)
```
We can inspect the distribution of number of keywords as a function of the day of the week. Across the top are the unique number of keywords in the channel.  Share are divided by number of keywords and the day the article is shared.


## Correlations

We want to remove any predictor variables that are highly correlated to the response variable which can cause multicollinearity.  If variables have this characteristic it can lead to skewed or misleading results. We can create groupings of  predictor variables to look at the relationship with our response variable, share and to each other.

```{r}
ggcorr(selectTrain%>% dplyr::select(n_tokens_title, n_tokens_content, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens,  shares),label_round = 2, label = "TRUE", label_size=3)
```

```{r}
ggcorr(selectTrain%>% dplyr::select( average_token_length, num_keywords,num_hrefs, num_self_hrefs, num_imgs, num_videos, shares),label_round = 2)
```


```{r}
ggcorr(selectTrain%>% dplyr::select(kw_min_min, kw_max_min, kw_avg_min, kw_min_max, kw_max_max,  kw_avg_max, kw_min_avg, kw_max_avg, kw_avg_avg, shares),label_round = 2, label = "TRUE")
```

```{r}
ggcorr(selectTrain%>% dplyr::select(self_reference_min_shares, self_reference_max_shares, self_reference_avg_sharess, global_subjectivity, global_sentiment_polarity, global_rate_positive_words, global_rate_negative_words, shares),label_round = 2, label = "TRUE")
```

```{r}
ggcorr(selectTrain%>% dplyr::select(LDA_00, LDA_01, LDA_02, LDA_03, LDA_04,  shares),label_round = 2, label = "TRUE")
```

```{r}
ggcorr(selectTrain%>% dplyr::select(rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity,  avg_negative_polarity, min_negative_polarity, max_negative_polarity, shares),label_round = 2, label = "TRUE")
```


```{r}
ggcorr(selectTrain%>% dplyr::select( title_subjectivity, title_sentiment_polarity, abs_title_subjectivity, abs_title_sentiment_polarity, shares),label_round = 2, label = "TRUE")
```
Looking at the results from the `ggcorr`, we do not see any highly correlated relationships with our response variable, share.  if there was such relationship, it would be 1 or orange for highly positive correlated and -1 or blue for highly negative correlated.   We do notice that many of the varaibles are highly correlated with one another.

**need to make corr plot look better - see if can change font on label**

# Varaible Selection

Before using any reduction algorithm to determine which variables to be in the model, I used the correlation tables above to reduce the predictors.  If the predictors are correlated, it's not necessary to similar predictor.

So I decided to remove min and max, if the predictor also contained age:
 min_positive_polarity
 max_positive_polarity
 min_negative_polarity
 max_negative_polarity
 self_reference_min_shares
 self_reference_max_shares
 kw_min_avg
 kw_max_avg
 kw_min_max
 kw_max_max
 kw_min_min
 kw_max_min
 
I removed all of the "LDA_".  When you view the correlation chart above, they seem to be correlated.  In addition, I am not sure what LDA means.  When I googled it, the results referenced modeling and Latent Dirichlet Allocation.
LDA_00: Closeness to LDA topic 0
LDA_01: Closeness to LDA topic 1
LDA_02: Closeness to LDA topic 2
LDA_03: Closeness to LDA topic 3
LDA_04: Closeness to LDA topic 4

I removed the following, because the data contains the absolute value of the same information:
title_subjectivity
title_sentiment_polarity

I removed the following, because of their strong relationship between global_rate_positive_words and global_rate_negative_words:
global_subjectivity
global_sentiment_polarity


I removed the following, because of their strong relationship between n_tokens_content:
n_unique_tokens
n_non_stop_unique_tokens
n_non_stop_words

The final data set will contain the following columns (features):
n_tokens_title
n_tokens_content
num_hrefs
num_self_hrefs
num_imgs
num_videos
average_token_length
num_keywords
kw_avg_min
kw_avg_max
kw_avg_avg
self_reference_avg_sharess
global_rate_positive_words
global_rate_negative_words
rate_positive_words
rate_negative_words
avg_positive_polarity
avg_negative_polarity
abs_title_subjectivity
abs_title_sentiment_polarity

So, that leaves us with 20. There is a relationship between num_hrefs and num_self_hrefs.  One of those may be removed later in the analysis.



# Linear Regression

Linear regression is a method to understand the relationship between a response variables Y and one or more predictor variables x, x1, x2, etc.  This method creates a line that best fits the data called "least known regression line".

For a Simple Linear Regression, we have one response variable and one predictor variables. It uses the formula y = b0 + b1x, where

    y:  response variable
    b0: intercept (baseline value, if all other values are zero)
    b1: 

For Multiple Linear Regression, we have one response variable and any number of predictor variables. It uses the formula Y = B0 + B1X1 + B2X2 + … + BpXp + E, where

    Y: The response variable
    Xp: The pth predictor variable
    Bp: The average effect on Y of a one unit increase in Xp, holding all other predictors fixed
    E: The error term

For the results of a linear regression model to be valid and reliable, the following must be true

    1. Linear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y. 
    2. Independence: The residuals are independent. 
    3. Homoscedasticity: The residuals have constant variance at every level of x.
    4. Normality: The residuals of the model are normally distributed.


###  Multiple Linear Regression using Best Stepwise

```{r}
set.seed(21)

lmFit1<- train(shares ~ ., data = selectTrain%>% dplyr::select(shares, n_tokens_title,
n_tokens_content,
num_hrefs,
num_self_hrefs,
num_imgs,
num_videos,
average_token_length,
num_keywords,
kw_avg_min,
kw_avg_max,
kw_avg_avg,
self_reference_avg_sharess,
global_rate_positive_words,
global_rate_negative_words,
rate_positive_words,
rate_negative_words,
avg_positive_polarity,
avg_negative_polarity,
abs_title_subjectivity,
abs_title_sentiment_polarity),
method = 'leapSeq',
preProcess = c("center", "scale"),
tuneGrid  = data.frame(nvmax = 1:20),
trControl = trainControl(method = "cv", number = 5)
)

lmFit1$results
```

```{r}
lmFit1$bestTune
```
```{r}
coef(lmFit1$finalModel, 5)
```



### Random Forest

Random forest is a decision tree method. It takes a desired number of bootstrapped samples from the data and builds a tree for each sample. While building the trees, if a split occurs, then only a random sample of trees are averaged.  The average of predictions are used to determine the final model

The random forest model is created using `rf` as the method and `mtry` for the tuning grid.

```{r}
rfFit1 <- train(shares ~. , data = selectTrain %>% dplyr::select(shares, n_non_stop_unique_tokens, kw_avg_max, self_reference_min_shares, n_tokens_content, num_hrefs,kw_max_avg, weekday,n_non_stop_words, num_videos, kw_avg_avg),
method = 'rf',
trControl = trainControl(method = 'cv', number = 3),
preProcess = c("center", "scale"),
tuneGrid  = data.frame(mtry = 1:10)    
)

rfFit1
```



# Automation of Building Six Reports 
```{r, eval = FALSE}
selectID <- unique(newData$channel)  

output_file <- paste0(selectID, "Analysis.html")   #should be .md for the project

params = lapply(selectID, FUN = function(x){list(channel = x)})

reports <- tibble(output_file, params)

library(rmarkdown)

apply(reports, MARGIN = 1,
      FUN = function(x){
        render(input = "./TestOne.Rmd",
               output_format = "github_document", 
               output_file = x[[1]], 
               params = x[[2]])
      })



```

------------------------------------------------------------
## Smitali 
SOW

Introduction Section 
Summary statistics each member to show contribution
1- Linear Regression - each member 
Boosted Tree Model (with CV)
Model comparsions (automation done by 1st member)
Any other support 